<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 梯度下降与镜像下降线性耦合 | Tingting Liao </title> <meta name="author" content="Tingting Liao"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tingtingliao.github.io/blog/2019/Linear_Coupling/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tingting</span> Liao </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">梯度下降与镜像下降线性耦合</h1> <p class="post-meta"> April 25, 2019 • Tintin </p> <p class="post-tags"> <a href="/blog/2019"> <i class="fa-solid fa-calendar fa-sm"></i> 2019 </a>   ·   <a href="/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> optimization</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <p>本文来源于<sup id="fnref:footnote" role="doc-noteref"><a href="#fn:footnote" class="footnote" rel="footnote">1</a></sup>Allen-Zhu的Gradient-Mirror Coupling，将\(MirrorDescent\)和\(GradientDescent\)组合起来设计了一个全新的加速方法，并将其扩展到不能用moment等加速方法的场景下。</p> <h3 id="1-online-learning-regret">1. Online Learning Regret</h3> <p>在线学习最主要的一个限制是只能看到当前和过去的数据，未来是未知的，有可能完全颠覆现在的认知。因此，在线学习所追求的是直到所有数据所能设计的最优策略。同这个最优策略的差异成为regret。</p> \[Regret_T(h^*) = \sum_{t=1}^T (l(h(x_t),y_t)-l(h^*(x_t),y_t))\] <p>\(l\)表示损失函数,\(h,h^*\)分别表示当前策略和最优策略。\(T\)表示当前第\(T\)个回合，online learning每个回合都会有一个新数据，随着时间增加数据变多，\(Regret\)也会变小。</p> <p>作者引入此概念用以描述求解过程中，当前更新的点\(x_t\)与最优点\(x^*\)之间的目标函数\(f(x)\)的差值：</p> \[\begin{split} R_k(x^*) \equiv \sum_{t=0}^{k-1} (f(x_t)-f(x^*))&amp; = \sum_{t=0}^{k-1} f(x_t)- T f(x^*) \\ &amp; = k f( \overline{x})- k f(x^*) \\ &amp; = k(f( \overline{x})-f(x^*) ) \end{split}\] <p>而根据Convexity：</p> \[\begin{split} f(x^*)&amp;\geq f(x)+ \left \langle \triangledown f(x),x^*-x \right \rangle \\ &amp;\geq \frac{1}{k}\sum_{t=0}^{k-1}f(x_t)+ \frac{1}{k}\sum_{t=0}^{k-1}\left \langle \triangledown f(x_t),x^*-x_t \right \rangle \\ &amp;\geq f(\overline{x})+\frac{1}{k}\sum_{t=0}^{k-1}\left \langle \triangledown f(x_t),x^*-x_t \right \rangle 【琴生不等式】 \\ f(\overline{x})-f(x^*)&amp; \leq \frac{1}{k}\sum_{t=0}^{k-1}\left \langle \triangledown f(x_t),x_t-x^* \right \rangle \end{split}\] <p>所以：</p> \[R_T(x^*) \equiv \sum_{t=0}^{T-1} (f(x_t)-f(x^*)) = T(f( \overline{x})-f(x^*) ) \leq \sum_{t=0}^{T-1}\left \langle \triangledown f(x_t),x_t-x^* \right \rangle \quad (1.1)\] <p>后面的收敛速率都是由该式推导而出的。只用求出\(f( \overline{x})-f(x^*) \leq \epsilon\)时所需步数\(T\)即可。</p> <h3 id="2-对偶问题镜像下降">2. 对偶问题：镜像下降</h3> <p><strong>【Mirror Descent】</strong>：</p> \[\widetilde{x}=Mirr_x(\alpha *\partial f(x))\] \[Mirr_x(\varepsilon )=\min_y \{D(x,y)+\left \langle \varepsilon,y-x\right \rangle \}\] <p>由上节可知，求收敛速度，很重要一步是求出\(\left \langle \triangledown f(x_t),x_t-x^* \right \rangle\),根据<strong>三点性质</strong>有：</p> \[\begin{split} ||x_{k+1}-x^*||^2 &amp;=||x_{k}-\alpha \triangledown f(x_k)-x^*||^2\\ &amp;= ||x_{k}-x^*||^2-2\alpha\triangledown f(x_k)(x_{k}-x^*)+\alpha^2||\triangledown f(x_k)||^2 \\ \end{split}\] \[\therefore \alpha \triangledown f(x_k)(x_{k}-x^*) = \frac{\alpha^2}{2}||\triangledown f(x_k)||^2 + D(x_k,x^*) -D(x_{k+1},x^*)\] <p>关于<strong>Bregman divergence</strong>的距离函数\(D(x,y)\)，详细解释见<a href="http://127.0.0.1:4000/2019/04/09/Proximal-Descent/" rel="external nofollow noopener" target="_blank">这里</a>.</p> <p>根据<font color="blue">(1.1)式</font>，\(MD\)中的\(Regret\)：</p> \[\begin{split} \alpha T(f( \overline{x})-f(x^*) ) &amp;\leq \alpha \sum_{t=0}^{T-1}\left \langle \triangledown f(x_t),x_t-x^* \right \rangle \\ &amp; \leq \frac{\alpha^2}{2}\sum_{k=0}^{T-1} ||\triangledown f(x_k)||^2 + D(x_0,x^*) -D(x_{T},x^*) \\ &amp; \leq \frac{\alpha^2}{2}\sum_{k=0}^{T-1} ||\triangledown f(x_k)||^2 + D(x_0,x^*) -D(x_{T},x^*) \end{split}\] <p>令\(\rho = \frac{1}{T}\sum_{k=0}^{T-1}\|\triangledown f(x_k)\|^2\)为梯度平方均值，那么：</p> \[\begin{split} \alpha T(f( \overline{x})-f(x^*)) \leq \frac{\alpha^2}{2} T \rho^2 + D(x_0,x^*) \quad (2.1) \end{split}\] <ul> <li>若\(f(x)\)为\(L-smooth\),取\(\alpha = \frac{1}{L}\),有：</li> </ul> \[f( \overline{x})-f(x^*) \leq \frac{\rho^2}{2L} + \frac{LD(x_0,x^*)}{T}\] <p>令\(\frac{LD(x_0,x^*)}{T}\leq \epsilon\)，收敛速率\(T \geq \frac{LD(x_0,x^*}{\epsilon})\)</p> <ul> <li>若\(f(x)\)为\(non-smooth\),令\(\alpha = \frac{\sqrt{2D(x_0,x^*)}}{\rho \sqrt{T}}\),由（2.1）式得：</li> </ul> \[f( \overline{x})-f(x^*) \leq \frac{2D(x_0,x^*)}{\alpha T} = \frac{\sqrt{2D(x_0,x^*)}· \rho}{ \sqrt{T}}\] <p>达到\(\epsilon\)精度，收敛速率\(T \geq \frac{2D(x_0,x^*) \rho^2}{\epsilon^2 }\)。</p> <p>在非光滑的情况下，\(MD\)的收敛速度是\(O(\frac{1}{\epsilon^2})\)，比\(GD\)慢一个量级，而在光滑的情况下两者都是\(O(\frac{1}{\epsilon})\)。</p> <h4 id="mirror-descent与gradient-descent联系">Mirror Descent与Gradient Descent联系</h4> <p>为了更好的\(MD\)与\(GD\)之间的联系，这里假设它们的目标函数都是\(L-smooth\)的：</p> \[f(x_{k+1}) \leq f(x_{k}) + \triangledown f(x_k)|x_{k+1}-x_k|+ \frac{L}{2}||x_{k+1}-x_k||^2\] \[\begin{split} GD(x)&amp;= argmin_y \{ \triangledown f(x)|y-x|+ \frac{L}{2}||y-x||^2\} \\ x_{k+1} &amp;\leftarrow x_k - \frac{1}{L} \triangledown f(x_k) \\ \\ \\ MD(\alpha\triangledown f(x))&amp;=argmin_y \{ \alpha\triangledown f(x)|y-x|+ D(x,y)\} \\ &amp;= argmin_y \{ \alpha\triangledown f(x)|y-x|+ \frac{L}{2}||y-x||^2\} \end{split} \\ x_{k+1} \leftarrow x_k - \alpha \frac{1}{L} \triangledown f(x_k)\] <p>\(GD\)的收敛定理是minimize一个quadratic upper bound<a href="https://tintin.space/2019/03/29/Convergence/" rel="external nofollow noopener" target="_blank">证明见这里</a>，而\(MD\)是minimize dual averaging lower bound.</p> \[f(x_k)-f(x^*)\leq \frac{L\|x_0-x^*\|^2}{2T} 【GD】\] \[\alpha T(f( \overline{x})-f(x^*) ) \leq \alpha \sum_{t=0}^{T-1}\left \langle \triangledown f(x_t),x_t-x^* \right \rangle \leq \frac{\alpha^2}{2}\sum_{k=0}^{T-1} ||\triangledown f(x_k)||^2 + D(x_0,x^*) -D(x_{T},x^*) 【MD】\] <p>当\(f(x)\)光滑，距离函数为欧式距离且\(\alpha=1\)时，\(MD\)和\(GD\)等价。由此我们也可以从一个high level角度解释\(MD\)和\(GD\)，当\(\|\triangledown f(x_k)\|\)特别大的时候，\(GD\)更新的距离也很大，而当\(\|\triangledown f(x_k)\|\)很小即函数很平缓时，相反\(MD\)会表现更好，此时它的\(Regret\)比较小bound也更加严格，而\(\alpha\)也是随着时间的变化在逐步调节获得更好的收敛。</p> <p>因此我们很容易想到，是否可以<font color="red">结合梯度下降和镜像下降得到一个更快速的一阶算法？</font></p> <h3 id="3-linear-coupling">3. Linear Coupling</h3> <p><img src="/img/LinearCoupling/Algorithm.png" alt=""></p> \[\begin{split} MD\&amp;GD \quad x_{k+1}&amp; \leftarrow \tau_k MD(x_{k})+(1-\tau_k)GD(x_{k}) \\ x_{k+1}&amp;\leftarrow \tau_k z_{k+1}+(1-\tau_k)y_{k+1} \end{split}\] <p>同样地，我们需要先求出\(\alpha \triangledown f(x_{k+1})(x_{k+1}-x^*)\),但Linear Coupling不是\(MD\),而是\(MD\)和\(GD\)的线性组合，因此我们需要将其拆开：</p> \[\alpha \triangledown f(x_{k+1})(x_{k+1}-x^*) = \alpha \triangledown f(x_{k+1})(x_{k+1}-z_{k}) +\alpha \triangledown f(x_{k+1})(z_{k}-x^*)\] <p><strong>第二项</strong>\(z_k\)满足\(MD\)(2.1)式:</p> \[\begin{split} \alpha \triangledown f(x_{k+1})(z_{k}-x^*) &amp;= \frac{\alpha^2}{2}||\triangledown f(x_{k+1})||^2 + D(z_k,x^*) -D(z_{k+1},x^*) \\ &amp; \leq \alpha^2 L (f(x_{k+1})-f(y_{k+1})) + D(z_k,x^*) -D(z_{k+1},x^*) \end{split}\] <p>\(L-smooth\)梯度下降中满足(<a href="https://tintin.space/2019/03/29/Convergence/" rel="external nofollow noopener" target="_blank">证明见这里</a>):</p> \[f(x_k)-f(x_{k+1})\geq \frac{1}{2L}||\triangledown f(x_k)||^2\] <p>Linear Coupling中\(y_{k+1}\)是\(x_{k+1}\)经过\(GD\)的下一步，因此：</p> \[f(x_{k+1})-f(y_{k+1})\geq \frac{1}{2L}||\triangledown f(x_{k+1})||^2\] \[\therefore 第二项 \leq \alpha^2 L (f(x_{k+1})-f(y_{k+1})) + D(z_k,x^*) -D(z_{k+1},x^*)\] <p><strong>第一项</strong>：</p> \[\begin{split} \alpha \triangledown f(x_{k+1})(x_{k+1}-z_{k}) =&amp; \alpha \triangledown f(x_{k+1})(x_{k+1}-\frac{x_{k+1}-(1-\tau_k)y_{k} }{\tau_k}) \\ =&amp; \alpha \triangledown f(x_{k+1}) \frac{1-\tau_k }{\tau_k}(y_{k}-x_{k+1})\\ \leq &amp; \alpha \frac{1-\tau_k }{\tau_k} (f(y_k) -f(x_{k+1})) 【一阶凸性质】 \end{split}\] <p>令\(\frac{1-\tau_k }{\tau_k} = \alpha L\),两式相加得:</p> \[\alpha \triangledown f(x_{k+1})(x_{k+1}-x^*) \leq \alpha^2 L (f(y_k)-f(y_{k+1})) + D(z_k,x^*) -D(z_{k+1},x^*)\] <p>根据<font color="blue">(1.1)式</font>, Linear Coupling的\(Regret\):</p> \[\begin{split} \alpha T(f( \overline{x})-f(x^*)) &amp;\leq \alpha \sum_{t=0}^{T-1}\left \langle \triangledown f(x_t),x_t-x^* \right \rangle \\ &amp;\leq \alpha^2 L (f(y_0)-f(y_{T})) + D(z_0,x^*) -D(z_{T},x^*) \\ f( \overline{x})-f(x^*) &amp;\leq \frac{1}{T} (\alpha L d + \frac{D(z_0,x^*)}{\alpha} ) \end{split}\] <p>其中\(d=f(y_0)-f(y_{T})\),令\(\alpha = \sqrt{\frac{ D(z_0,x^*)}{Ld}}\),</p> \[f( \overline{x})-f(x^*) \leq \frac{2\sqrt{LdD(z_0,x^*)}}{T}\] <p>当\(T = 4\sqrt{\frac{LD(z_0,x^*)}{d}}\)时，\(f( \overline{x})-f(x^*) \leq \frac{d}{2}\),即当前点的距离到最优点距离变成一半.即每个epoch的距离依次为\(d,\frac{d}{2},\frac{d}{4},...\),那么收敛速率：</p> \[T = O(\sqrt{\frac{LD(z_0,x^*)}{\epsilon}}+\sqrt{\frac{LD(z_0,x^*)}{2\epsilon}}+\sqrt{\frac{LD(z_0,x^*)}{4\epsilon}}+...) =O(\sqrt{\frac{LD(z_0,x^*)}{\epsilon}})\] <p>Linear Coupling的收敛速度\(\frac{1}{\sqrt{\epsilon}}\)和Nesterov加速方法一样，比\(GD\)快了一个量级。其中步长是固定的\(\alpha = \sqrt{\frac{ D(z_0,x^*)}{Ld}}\)随着时间的增加\(\alpha\)变大，我们取\(\tau=\frac{1}{\alpha L+1}\)逐渐减小，也就是说随着当前点离最优点越来越近的时候，\(\alpha\)变小，\(\tau\)变大，\(GD\)更新能力逐渐变弱而\(MD\)逐渐变强。</p> <p>但是它有三个缺点：1）\(\alpha\)的值取决于\(D(z_0,x^*)\);2)\(d\)初始化好坏会影响收敛；3）算法需要重新开始，即在每个epoch开始时都需要重新初始化参数。</p> <p>为了解决这几个问题，Zheyuan将\(\alpha，\tau\)设为随着时间变化的参数不需要额外初始化其它参数,\(\alpha_{t+1} = \frac{t+2}{2L},\tau_t=\frac{1}{\alpha L+1}=\frac{2}{t+2}\),收敛速率为\(O(\sqrt{\frac{4LD(z_0,x^*)}{\epsilon}})\)</p> <p>优点：1）Linear Coupling不同于常见的momentum加速方法，但是它和一般的加速方法有着相同的收敛速度； 2）利用Mirror Descent应用于非Euclidean Distance场景中。</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:footnote" role="doc-endnote"> <p><a href="https://arxiv.org/pdf/1407.1537.pdf" rel="external nofollow noopener" target="_blank">Allen-Zhu Z, Orecchia L. Linear coupling: An ultimate unification of gradient and mirror descent[J]. arXiv preprint arXiv:1407.1537, 2014.</a>. <a href="#fnref:footnote" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/VarienceReduction/">Variance Reduction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/Proximal-Descent/">近端梯度下降</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/Convergence/">梯度下降收敛速度</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/MeanFieldTheory/">Mean Field Theory Solution of the Ising Model</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/Katyusha/">Katyusha</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Tingting Liao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>