<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 近端梯度下降 | Tingting Liao </title> <meta name="author" content="Tingting Liao"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png?cf7d278f0f40bdd17f7c63df46d33eb3"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tingtingliao.github.io/blog/2019/Proximal-Descent/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tingting</span> Liao </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">近端梯度下降</h1> <p class="post-meta"> April 09, 2019 • Tintin </p> <p class="post-tags"> <a href="/blog/2019"> <i class="fa-solid fa-calendar fa-sm"></i> 2019 </a>   ·   <a href="/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> optimization</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <h3 id="1-满足lipschitz连续的梯度下降">1. 满足Lipschitz连续的梯度下降</h3> \[\begin{split} f(x^{k+1})&amp; \geq f(x^k)+\left \langle f'(x^k),x^{k+1}-x^k\right \rangle+\frac{L}{2}||x^{k+1}-x^k||^2\\ &amp;\geq \frac{L}{2}||(x^{k+1}-x^k)+\frac{1}{L}f'(x^k))||^2+\Phi(x^k) \end{split}\] <p>将不等式右边拆分成关于\(x^{k+1}\)的项，其它关于\(x^k\)为无关项写成\(\Phi(x^k)\)</p> \[\begin{split} &amp;\min_{x^{k+1}} f(x^{k+1})\\ = \quad&amp; \min ||(x^{k+1}-x^k)+\frac{1}{L}f'(x^k))||^2\\ \therefore \quad &amp;x^{k+1} = x^k - \frac{1}{L}f'(x^k) \end{split}\] <p>在满足Lipchitz连续的下降算法中，步长取\(t=\frac{1}{L}\)，在上一篇博客中也有写到,将该函数推广到任意可导函数，步长为\(\alpha\)，即为一般的梯度下降。</p> <h3 id="2-近端梯度下降proximal-gradient-descent">2. 近端梯度下降Proximal gradient descent</h3> <p>\(PG\)主要针对损失函数中有不可导的函数的梯度下降问题，其中\(h(x)\)为不光滑的凸函数如\(L_1\)正则.</p> \[\min_x f(x)+ \lambda h(x)\] <p>根据上节推导有：</p> \[\begin{split} f(x^{k+1})=\frac{L}{2}||(x^{k+1}-x^k)+\frac{1}{L}f'(x^k))||^2+\Phi(x^k) +\lambda ||x^{k+1}||_1 \end{split}\] <p>令\(z = x^k - \frac{1}{L}f'(x^k)\)，则优化变成：</p> \[x^{k+1} = prox_{\lambda h(x)}=\min_x \frac{1}{2t}||x-z||^2+\lambda ||x||\] <p>这里将\(z\)看作常数，根据求\(f(x)\)的导数得到\(z\),然后目标变成找一个最优的\(x^*\)使得上式最小。即0属于上式导数的区间内【不理解这句话建议先看次梯度】：</p> \[0 \in \frac{1}{t}(x^*-z)+\lambda \partial |x^*|\] <p>其中： \(\partial |x^*|= \left\{\begin{matrix} 1 \quad &amp;x^*&gt;0\\ [-1,1] \quad &amp;x^*=0\\ -1 \quad &amp;x^*&lt;0 \end{matrix}\right.\)</p> <p>那么：</p> \[\begin{split} &amp;x^*&gt;0，0=\frac{1}{t}(x^*-z)+\lambda，\therefore x^*=z-t\lambda&gt;0 \\ &amp;x^*&lt;0，0=\frac{1}{t}(x^*-z)-\lambda，\therefore x^*=z+t\lambda&lt;0 \\ &amp;x^*=0，0 \in [\frac{1}{t}(x^*-z)-\lambda,\frac{1}{t}(x^*-z)+\lambda]，\therefore x^*=0 \end{split}\] <p>因此：</p> \[x^* =\left\{\begin{matrix} z-t\lambda \quad &amp;z&gt;t\lambda \\ 0 \quad &amp;-t\lambda&lt;z&lt;t\lambda \\ z+t\lambda \quad &amp;z&lt;-t\lambda \end{matrix}\right.\] \[prox_{\lambda h(x)}=\max\{|z|-t\lambda,0\}\] <h5 id="总结">总结</h5> <p>近端梯度下降主要分为两步：1）沿着\(f(x)\)梯度方向找到点\(z\)；2)优化\(h(x)\)</p> \[\begin{split} &amp;Gradient Step:z = x^k - tf'(x^k) \\ &amp;Proximal Step:x_{k+1} = \max\{|z|-t\lambda,0\} \end{split}\] <p><img src="https://pic4.zhimg.com/80/v2-1edbeea9078c5d42ad6c85fe9c2d3203_hd.jpg" width="500"></p> <p>该图中所表示的意思其实是：</p> \[\begin{split} &amp;\min_x \frac{1}{2\alpha}||x-z||^2+h(x) \\ \Rightarrow&amp; \frac{1}{\alpha}(x-z)+\partial h(x)=0 \end{split}\] <p>即\(x\)为\(\left\{\begin{matrix} y_1&amp;= -\frac{1}{\alpha}(x-z)\\ y_2&amp;=\partial h(x) \end{matrix}\right.\)两线交点，\(y_1\)为图中斜率为\(-\frac{1}{\alpha}\)斜线，该线经过点\((x_k, -\frac{\triangledown f(x_k)}{\alpha})\)【图中\(- \triangledown f(x)应写成-\frac{\triangledown f(x)}{\alpha}\)】，\(y_1\)与横坐标的交点即为Gradient Step的\(z\)，与\(y_2=\partial h(x)\)为Proximal Step最终的解.</p> <p>近端梯度下降的收敛速度与梯度下降类似，假设\(f(x)\)满足\(L-Lipschitz\)且\(h(x)\)为凸函数，固定步长\(t\),\(PG\)满足：</p> \[f(x^k)-f(x^*)\leq \frac{||x_0-x^*||_2}{2tk}\] <p>收敛速度为\(O(\frac{1}{\epsilon})\)</p> <h3 id="3-投影梯度下降projected-gradient-descent">3. 投影梯度下降Projected gradient descent</h3> <p>对于有约束的凸函数：</p> \[\min_{x\in S} f(x)\] <p>每次梯度下降需要将计算后的值拉回到约束域\(S\)中去</p> \[y_{k+1} = x_k - \eta \triangledown f(x_k)\] \[x_{k+1} = \min_{x\in S} ||x-y_{k+1}||^2_2\] <p><img src="http://freemind.pluskid.org/att/2014/06/proj-grad-descent.png" width="200"></p> <p>若梯度更新后的值不在\(S\)内，那么将其拉回到\(S\)中最近的点,即图中的\(\overline{x}_k\)。</p> <p>\(PD\)的第二步优化可以近似的看成是一次投影,找到一个距离\(z\)较近的点，使得\(h(x)\)尽可能的小。</p> \[\min_x \frac{1}{2t}||x-z||^2+h(x)\] \[\min D(x,z)+h(x)\] <h2 id="4-镜像梯度下降mirror-descent">4. 镜像梯度下降Mirror descent</h2> <p>投影算法和\(PD\)都是计算欧式距离，而镜像梯度下降的贡献主要在于定义了一个更一般的距离而不仅仅是欧式距离我们称之为Bregman散度，它包含了对一切距离的定义。</p> <p><strong>Bregman divergence</strong> 以下来源于<a href="https://www.zhihu.com/question/22426561/answer/209945856" rel="external nofollow noopener" target="_blank">这里</a>。欧式距离：</p> \[\begin{split} d^2(x,y)&amp;=\sum_{i=1}^{n}(x_i-y_j)\\ d^2(x,y)&amp;=||x||^2-||y||^2-\left \langle 2y,x-y\right \rangle \end{split}\] <p>令\(f(x)=x^2\)时，上式变为:</p> \[B(x,y)=f(x)-(f(y)+\left \langle \triangledown f(y),x-y\right \rangle)\] <p><img src="https://pic1.zhimg.com/80/v2-fe60d85a273c0ee712852e4c9375a0ff_hd.jpg" width="300"></p> <p>\(f(y)+\left \langle \triangledown f(y),x-y\right \rangle)\)为函数\(f(x)=x^2\)在\(y\)点切线在\(x\)处的取值，所以欧式距离可以表示为图中蓝线部分。而距离必须满足非负\(d(x,y)\geq 0\),那么\(f\)必须在其切线的上方,也就是说函数\(f\)是凸函数.<br> \(\quad\)因此我们可以将欧式距离函数\(f(x)=x^2\)换成其它距离的凸函数，如: <img src="https://pic2.zhimg.com/80/v2-cbbe99689224cdd829003483938af50e_hd.jpg" width="800"></p> <p>【\(Mirror Descent\)三点性质】:</p> \[\begin{split} ||x_{k+1}-x^*||^2 &amp;=||x_{k}-t\triangledown f(x_k)-x^*||^2\\ &amp;= ||x_{k}-x^*||^2-2t\triangledown f(x_k)(x_{k}-x^*)+t^2||\triangledown f(x_k)||^2 \\ \end{split}\] <p>因为\(t \triangledown f(x_k) = x_{k}-x_{k+1}\)</p> \[\triangledown f(x_k)(x_k -x^*) = \frac{1}{2t}( ||x_{k}-x^*||^2 - ||x_{k+1}-x^*||^2+ ||x_{k}-x_{k+1}||^2 ) \quad (1)\] <p>将左边的\(x_k\)换成\(x_k = x_{k+1}+t\triangledown f(x_k)\),该式可变为：</p> \[\triangledown f(x_k)(x_{k+1} -x^*) = \frac{1}{2t}( ||x_{k}-x^*||^2 - ||x_{k+1}-x^*||^2-||x_{k}-x_{k+1}||^2 ) \quad (2)\] <p>这就是欧式距离的\(Bregman Divergence\)的三点性质。【(2)对应SVRG++定理3.2】</p> <p>若\(f(x)\)为凸函数，（2）式可写成：</p> \[f(x_{k+1})-f(x^*) \leq \frac{1}{t}(D(x_k,x^*)-D(x_{k+1},x^*)-D(x_k,x_{k+1}))\] \[f(x_{k+1})+ \frac{1}{t}(D(x_{k+1},x_k)+D(x_{k+1},x^*)) \leq f(x^*)+ \frac{1}{t}D(x^*,x_k)\] <p>该式对应<a href="https://zhuanlan.zhihu.com/p/34299990" rel="external nofollow noopener" target="_blank">这里</a>引理1：</p> \[\phi (z^+) + D(z^+,z)+D(x,z^+) \leq \phi(x) + D(x,z)\] <p>其中\(z^+\)为\(z\)的下一步，\(x^*\)对应\(x\),然后即可推出mirror descent达到\(\epsilon\)误差需要\(\frac{LD(x^*,x_0)}{\epsilon}\),上篇文章<a href="https://tintin.space/2019/03/29/Convergence/" rel="external nofollow noopener" target="_blank">收敛速度</a>中推导出梯度下降:</p> \[f(x_k)-f(x^*) \leq \frac{||x_0-x^*||^2}{2tk}\] <p>达到\(\epsilon\)误差需要走\(\frac{\|x_0-x^*\|^2}{2t\epsilon}\)步,步长取\(t=\frac{1}{L}\),\(D\)取欧式距离，mirror descent收敛速度与GD收敛速度一样。</p> <p>同样地，对于非光滑mirror descent用次梯度，需要步数：</p> \[k=\frac{L^2 \|x_0-x^*\|^2 }{2\epsilon^2} =\frac{2L^2D(x_0,x^*)}{\epsilon^2}\] </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/Convergence/">梯度下降收敛速度</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/Linear_Coupling/">梯度下降与镜像下降线性耦合</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/VarienceReduction/">Variance Reduction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/Primal/">Primal Dual Problem</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/MeanFieldTheory/">Mean Field Theory Solution of the Ising Model</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Tingting Liao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>